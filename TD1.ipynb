{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning and Computer vision\n",
    "\n",
    "### Overfitting, Regularization and Cross-validation\n",
    "\n",
    "Shani Israelov\n",
    "\n",
    "Jean Monnet University, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this introduction exercise, we are going to work on a polynomial ridge regression task. The idea is\n",
    "to understand the notions of overfitting, regularization and cross-validation.\n",
    "We have some points in a 2D space (X,y), which are our training data. We want to learn a polynomial\n",
    "function P() that predicts the y from the X : y = P(X).\n",
    "The test data will beprovided when you think you have a « good » predictor.\n",
    "The optimization step can not be changed, you can only play with two hyper-parameters: the order\n",
    "of the polynomial function and the regularization term.\n",
    "You can use your own python environment or just run your code online on trinket\n",
    "(https://trinket.io/python3)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Read, understand and run the provided code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score : 0.71818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Data\n",
    "X=[[-3], [-2.5], [-2], [-1.5], [-1], [-0.5], [0], [0.5], [1], [1.5], [2]]\n",
    "y=[-0.303, -0.545, -1.025, -0.959, -0.768, -0.375, -0.021, 0.438, 0.883, 0.807, 0.932]\n",
    "\n",
    "# Hyperparameters\n",
    "degree=1\n",
    "regul_param=0\n",
    "\n",
    "# Model\n",
    "model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "\n",
    "# Training\n",
    "model.fit(X, y)\n",
    "\n",
    "# Testing\n",
    "ypred=model.predict(X)\n",
    "r2 = r2_score(y,ypred)\n",
    "print(\"Training score : %0.5f\" % (r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-Notes:\n",
    "##### make_pipeline\n",
    "Construct a Pipeline from the given estimators.\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n",
    "\n",
    "##### PolynomialFeatures(degree)\n",
    "Generate polynomial and interaction features.\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "\n",
    "##### Ridge(alpha=regul_param))\n",
    "Linear least squares with l2 regularization. Minimizes the objective function:\n",
    "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
    "This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
    "\n",
    "from sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ What is the R2 score (maximum, zero value) ?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to understand these metrics in order to determine whether regression models are accurate or misleading. \n",
    "In terms of linear regression, variance is a measure of how far observed values differ from the average of predicted values, i.e., their difference from the predicted value mean. The goal is to have a value that is low. \n",
    "The r2 score varies between 0 and 100%. It is closely related to the MSE, but not the same. \n",
    " “(total variance explained by model) / total variance.”\n",
    "A low value would show a low level of correlation, meaning a regression model that is not valid, but not in all cases.\n",
    "from: https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/\n",
    "\n",
    "regression score function.\n",
    "Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). In the general case when the true y is non-constant, a constant model that always predicts the average y disregarding the input features would get a score of 0.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ For the orders from 1 to 8, which polynomial function provides the best training results\n",
    "(regularization coefficient = 0, here) ? Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree : 0.000000, Training score : 0.00000\n",
      "Degree : 1.000000, Training score : 0.71818\n",
      "Degree : 2.000000, Training score : 0.85192\n",
      "Degree : 3.000000, Training score : 0.98302\n",
      "Degree : 4.000000, Training score : 0.98515\n",
      "Degree : 5.000000, Training score : 0.99111\n",
      "Degree : 6.000000, Training score : 0.99114\n",
      "Degree : 7.000000, Training score : 0.99882\n",
      "Degree : 8.000000, Training score : 0.99898\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    # Hyperparameters\n",
    "    degree=i\n",
    "    regul_param=0\n",
    "    # Model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "    # Training\n",
    "    model.fit(X, y)\n",
    "    # Testing\n",
    "    ypred=model.predict(X)\n",
    "    r2 = r2_score(y,ypred)\n",
    "    print(\"Degree : %f, Training score : %0.5f\" % (degree, r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get the highest score for degree 8, it makes sense cause we have 10 data samples and 8 is probabley overfitted to the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/ By fixing the order to 5, play with the regularization coefficient from 1e-4 to 10. What is the\n",
    "impact on the training result ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization coefficient : 0.000100, Training score : 0.99111\n",
      "Regularization coefficient : 0.001000, Training score : 0.99110\n",
      "Regularization coefficient : 0.010000, Training score : 0.99109\n",
      "Regularization coefficient : 0.100000, Training score : 0.99013\n",
      "Regularization coefficient : 1.000000, Training score : 0.95363\n",
      "Regularization coefficient : 10.000000, Training score : 0.80067\n"
     ]
    }
   ],
   "source": [
    "regul_coeffs = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "for i in range(len(regul_coeffs)):\n",
    "    # Hyperparameters\n",
    "    degree=5\n",
    "    regul_param=regul_coeffs[i]\n",
    "    # Model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "    # Training\n",
    "    model.fit(X, y)\n",
    "    # Testing\n",
    "    ypred=model.predict(X)\n",
    "    r2 = r2_score(y,ypred)\n",
    "    print(\"Regularization coefficient : %f, Training score : %0.5f\" % (regul_param, r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the more the regularization coefficient is bigger than the training score is lower. \n",
    "regularization technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.Regularization, significantly reduces the variance of the model, without substantial increase in its bias.\n",
    "from: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "\n",
    "if the alpha is bigger, it means that the regularization term has more weight, more penalty to errors, the loss function is bigger, means the error is bigger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ Use the « cross_val_predict » function to run a cross validation. What would be a good number of\n",
    "folds, here ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds Number: 8.000000, Training score : 0.93221821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "degree = 5\n",
    "regul_param = 0.1\n",
    "cv = 8\n",
    "model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "model.fit(X, y)\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv) # cv is the number of folds, default is 5 \n",
    "r2 = r2_score(y,y_pred)\n",
    "print(\"Folds Number: %f, Training score : %0.8f\" % (cv, r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross val predict \n",
    "\n",
    "Generate cross-validated estimates for each input data point.\n",
    "The data is split according to the cv parameter (int, number of folds). Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.\n",
    "Passing these predictions into an evaluation metric may not be a valid way to measure generalization performance. Results can differ from cross_validate and cross_val_score unless all tests sets have equal size and the metric decomposes over samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/ Test a cross validation prediction with an order of 5 and a regularization of 0.1. Observe the result.\n",
    "How many values contains this vector ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/ Use the cross-validation to find the best hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 3.000000, Regularization: 0.000100, cv: 8.000000, Training score : 0.96958647\n"
     ]
    }
   ],
   "source": [
    "regul_coeffs = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "max_r2 = 0\n",
    "for i in range(8):\n",
    "    for j in range(len(regul_coeffs)):\n",
    "        degree = i+1\n",
    "        regul_param = regul_coeffs[j]\n",
    "        cv = 8\n",
    "        model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "        model.fit(X, y)\n",
    "        y_pred = cross_val_predict(model, X, y, cv=cv) # cv is the number of folds, default is 5 \n",
    "        r2 = r2_score(y,y_pred)\n",
    "        if r2 > max_r2:\n",
    "            max_r2 = r2\n",
    "            best_degree = degree\n",
    "            best_regul_param = regul_param\n",
    "\n",
    "print(\"Degree: %f, Regularization: %f, cv: %f, Training score : %0.8f\" % (best_degree, best_regul_param, cv, max_r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/ Test your solution on the the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score : 0.98749\n"
     ]
    }
   ],
   "source": [
    "degree = 3\n",
    "regul_param = 0.0001\n",
    "model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=regul_param))\n",
    "model.fit(X, y)\n",
    "Xtest=[[-3.2], [-2.2], [-1.2], [-0.2], [0.8], [1.8], [2.8]]\n",
    "ytest=[0.058, -0.808, -0.932, -0.199, 0.717, 0.974, 0.335]\n",
    "ytestpred=model.predict(Xtest)\n",
    "r2 = r2_score(ytest, ytestpred)\n",
    "print(\"Training score : %0.5f\" % (r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ce47280533a050555d7b4f68eeea0130aedc5e28a0a82ffba1c2960ac9c248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
